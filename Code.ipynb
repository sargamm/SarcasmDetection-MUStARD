{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Final Code file",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Mh2wsCyGxx5",
        "outputId": "55e2121b-9472-4f6f-90e1-b7b6a4dd2522"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObFSIXFClk88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1a8f4d3-a463-4c83-835b-43e0285b5e3e"
      },
      "source": [
        "cd /content/drive/MyDrive/NLP Project/Random Forest"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1owyyAUs7lGGypmp5pg74v7uwptu-YPm9/NLP Project/Random Forest\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPGARfAyPL2r",
        "outputId": "5aa516b9-267c-4771-940f-85e2f965d5b3"
      },
      "source": [
        "!pip install vaderSentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "!pip install simpletransformers\n",
        "!pip install h5py\n",
        "import h5py\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from nltk.corpus import subjectivity\n",
        "from nltk.sentiment import SentimentAnalyzer\n",
        "from nltk.sentiment.util import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import model_selection as md\n",
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
        "from simpletransformers.language_representation import RepresentationModel\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# import pandas as pd\n",
        "import logging\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.6/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: simpletransformers in /usr/local/lib/python3.6/dist-packages (0.49.7)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2019.12.20)\n",
            "Requirement already satisfied: transformers>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (3.5.1)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.9.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.18.5)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.10.11)\n",
            "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.22.2.post1)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.71.0)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (4.54.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.4.1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.2.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->simpletransformers) (3.12.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->simpletransformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->simpletransformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->simpletransformers) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->simpletransformers) (0.8)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->simpletransformers) (0.1.91)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (1.15.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (5.4.8)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (1.0.1)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (2.3)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (3.5.4)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (7.1.2)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (0.19.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (3.13)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (2.8.1)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (5.0.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (0.4.0)\n",
            "Requirement already satisfied: watchdog>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (0.10.4)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (3.1.11)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->simpletransformers) (0.17.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (1.16.25)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (7.0.0)\n",
            "Requirement already satisfied: botocore>=1.13.44 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (1.19.25)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (0.18.1)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (4.1.1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (0.14.1)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (4.1.0)\n",
            "Requirement already satisfied: blinker in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (1.4)\n",
            "Requirement already satisfied: base58 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (2.0.1)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (0.8.1)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (5.1.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (0.10.2)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (0.5.0)\n",
            "Requirement already satisfied: enum-compat in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (0.0.3)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (1.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (2020.11.8)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->simpletransformers) (2018.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers>=3.0.2->simpletransformers) (50.3.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=3.0.2->simpletransformers) (2.4.7)\n",
            "Requirement already satisfied: pathtools>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from watchdog>=0.8.3->wandb->simpletransformers) (0.1.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from GitPython>=1.0.0->wandb->simpletransformers) (4.0.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->streamlit->simpletransformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->streamlit->simpletransformers) (0.10.0)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from validators->streamlit->simpletransformers) (4.4.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (2.11.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (2.6.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.11.1)\n",
            "Requirement already satisfied: ipykernel>=5.1.2; python_version >= \"3.4\" in /usr/local/lib/python3.6/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (5.3.4)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.6/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (4.3.3)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.6/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (7.5.1)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb->simpletransformers) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->altair>=3.2.0->streamlit->simpletransformers) (1.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.3.5)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.5.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.5.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.0.8)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.7.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (20.0.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.0.18)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.5)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.5.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.9.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.4.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.2.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.4.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.5.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.15.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx2YBfcCaNEp"
      },
      "source": [
        "seed = 50 #setting up random seed value\n",
        "random.seed(seed)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfoOLhznbUUh"
      },
      "source": [
        "df = pd.read_json('/content/drive/MyDrive/NLP Project/MUStARD/data/sarcasm_data.json').transpose() #reading utterances from json file\n",
        "df['sarcasm']=df['sarcasm'].apply(lambda x: 1 if x==True else 0)\n",
        "df['sarcasm'].value_counts()\n",
        "df['index']=df.index.astype(str)\n",
        "df['index']=df['index'].apply(lambda x: x[:1]+\"_\"+x[1:]) #changing ids for utterances in a form carried over in audio and video feature files\n",
        "df=pd.get_dummies(df,columns=['speaker'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpZTqH3rsvzo"
      },
      "source": [
        "def preprocessing(data): #preproceesing text\n",
        "    data=data.lower()\n",
        "    contractions = { \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he had\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so is\",\n",
        "\"that'd\": \"that had\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you had\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}\n",
        "    for i in contractions: #replacing all contractions with their expanded forms\n",
        "        data=data.replace(i,contractions[i])\n",
        "    data=data.replace(r\"[^?!,a-zA-Z0-9 ]\",\"\")\n",
        "    \n",
        "    return data"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgyKDCe8sx8X"
      },
      "source": [
        "df[\"utterance\"]=df[\"utterance\"].apply(preprocessing) #applying preprocessing on utterances\n",
        "df['context']=df['context'].apply(lambda x: [preprocessing(i) for i in x]) #applying preprocessing on context dialogues\n",
        "df['context']=df['context'].apply(lambda x: ' '.join(x))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX9Or_Nw6vm0"
      },
      "source": [
        "def getFeatures(x): #getting a fixed length audio features\n",
        "  features = [0]*5094\n",
        "  i = 0\n",
        "  for x1 in x[0]:\n",
        "    for x2 in x1:\n",
        "      features[i] = x2\n",
        "      i = i+1\n",
        "  return features"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSdU6oVH3jgF"
      },
      "source": [
        "def get_img(X): #generating flattened visual features of fixed size 265000 for all datapoints\n",
        "  keys=list(X[:,5])\n",
        "  img_embed=[]\n",
        "  for i in keys:\n",
        "    key=str(i)\n",
        "    n1=hf.get(key)\n",
        "    n1=np.array(n1).flatten() #flattening vectors\n",
        "    if n1.shape[0]<265000:\n",
        "      result = np.zeros(265000)\n",
        "      result[:n1.shape[0]] = n1 #padding with 0 if size <265000\n",
        "      n1=result\n",
        "    else:\n",
        "      n1=n1[:265000] #slicing to 265000 if size greater than 265000\n",
        "    n1=list(n1)\n",
        "    img_embed.append(n1)\n",
        "  return np.array(img_embed)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUX8chY09qBc"
      },
      "source": [
        "def get_audio(X): #helps in audio feature generation of feature vectors\n",
        "  keys=X[:,5]\n",
        "  audio=[]\n",
        "  for i in keys:\n",
        "    audio.append(np.array(df_audio.loc[i]))\n",
        "  return np.array(audio)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVuXqdS06ubS"
      },
      "source": [
        "df_audio = pd.read_pickle('/content/drive/MyDrive/NLP Project/MUStARD/data/audio_features.p') #importing audio features in their raw forms\n",
        "df_audio = pd.DataFrame.from_dict(df_audio, orient='index')\n",
        "df_audio=df_audio.apply(getFeatures, axis=1, result_type='expand')\n",
        "hf = h5py.File('/content/drive/MyDrive/NLP Project/MUStARD/data/features/utterances_final/resnet_pool5.hdf5', 'r') #importing visual features in their raw expanded forms\n",
        "df['visual']=list(get_img(np.array(df)))\n",
        "df['audio']=list(get_audio(np.array(df)))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQuAbCyLPo2B"
      },
      "source": [
        "def vaderScores(sentence): #generating vader sentiment scores\n",
        "  obj = SentimentIntensityAnalyzer()\n",
        "  v = obj.polarity_scores(sentence)\n",
        "  sentiment = []\n",
        "  sentiment.append(v['pos'])\n",
        "  sentiment.append(v['neg'])\n",
        "  sentiment.append(v['neu'])\n",
        "  sentiment.append(v['compound'])\n",
        "\n",
        "  return sentiment"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvVp2E7DCtyc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60afe0ae-5f13-4c19-dba9-62f04f42d883"
      },
      "source": [
        "X=df.drop(['sarcasm'],axis=1)\n",
        "Y=df['sarcasm']\n",
        "tfidf_vec_ut = TfidfVectorizer(ngram_range=(1, 2), max_features=500000, min_df=2)\n",
        "tfidf_vec_ut.fit(X['utterance']) #generating tf-idf ngram embeddings for utterances\n",
        "tfidf_vec_context = TfidfVectorizer(ngram_range=(1, 2), max_features=500000, min_df=2)\n",
        "tfidf_vec_context.fit(X['context']) #generating tf-idf ngram embeddings for context"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=1.0,\n",
              "                max_features=500000, min_df=2, ngram_range=(1, 2), norm='l2',\n",
              "                preprocessor=None, smooth_idf=True, stop_words=None,\n",
              "                strip_accents=None, sublinear_tf=False,\n",
              "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
              "                vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLo8TKweQinm"
      },
      "source": [
        "def get_sentiment(text): #fetching vader sentiment scores\n",
        "  sentiment_scores=[]\n",
        "  for i in text:\n",
        "    sentiment_scores.append(vaderScores(i))\n",
        "  return np.array(sentiment_scores)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBoEYpuUP1hK"
      },
      "source": [
        "In the following function, we continuously included or secluded code lines for inclusion and seclusion of different features. We have implemented the following features for various comparisons:  \n",
        "* Text(T)\n",
        "* Audio(A)\n",
        "* Visual(V)\n",
        "* T+V\n",
        "* T+A\n",
        "* A+V\n",
        "* T+A+V"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMCaRQrBSnWb"
      },
      "source": [
        "def create_feature_vec(X):\n",
        "  tfidf_ut=tfidf_vec_ut.transform(X[:,0]) # tf-idf utterances embedding\n",
        "  tfidf_context=tfidf_vec_context.transform(X[:,1]) #tf-idf context utterance embeddings\n",
        "  speaker=np.array(X[:,5:X.shape[1]-2]) #speaker one hot encoding vector\n",
        "  visual=np.array(X[:,X.shape[1]-2]) #visual flattened vector of length 265000\n",
        "  audio=np.array(X[:,X.shape[1]-1]) #audio features using libroso\n",
        "  sentiment_scores=get_sentiment(X[:,0]) #vader sentiment scores for utterances\n",
        "  sentiment_scores_con=get_sentiment(X[:,1]) #vader sentiment scores for context utterances\n",
        "  feat_vec=[np.concatenate((tfidf_ut[i].toarray()[0],tfidf_context.toarray()[0],speaker[i],visual[i],audio[i],sentiment_scores[i],sentiment_scores_con[i])) for i in range(X.shape[0])] #computing final vector\n",
        "  return feat_vec"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3cb_eb08PvK"
      },
      "source": [
        "def cross_val_helper(clf,X,y,train,test):\n",
        "  train_df = np.array(X)[train]\n",
        "  test_df = np.array(X)[test]\n",
        "  Y=np.array(y.astype(int))\n",
        "  Y_train=Y[train]\n",
        "  Y_test=Y[test]\n",
        "  feat_vec=np.array(create_feature_vec(train_df))\n",
        "  clf.fit(feat_vec,Y_train) #fitting our final model\n",
        "  feat_vec_test=np.array(create_feature_vec(test_df))\n",
        "  predictions = clf.predict(feat_vec_test) #predicting values using our final model\n",
        "  CM= confusion_matrix(predictions, Y_test).ravel()\n",
        "  print(CM)\n",
        "  scores=precision_recall_fscore_support(Y_test,predictions, average='weighted')\n",
        "  print(\"Precision: \" + str(scores[0]))\n",
        "  print(\"Recall: \" + str(scores[1]))\n",
        "  print(\"F1: \"+ str(scores[2]))\n",
        "  return [scores[0],scores[1],scores[2],clf]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BhHV0jIpeez"
      },
      "source": [
        "n=3\n",
        "kf = KFold(n_splits=n, random_state=seed, shuffle=True) #K-fold splits\n",
        "train_kf=[]\n",
        "test_kf=[]\n",
        "for train, test in kf.split(X):\n",
        "  train_kf.append(train)\n",
        "  test_kf.append(test)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VVvaXlDrRDF"
      },
      "source": [
        "Precision_RFC=[]\n",
        "Recall_RFC=[]\n",
        "F1_RFC=[]\n",
        "models=[]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jainxy_OftW"
      },
      "source": [
        "We have shown only the implementing code for Random Forest but same function has been used for fitting and testing other models as well in a similar manner. The models implemented are:  \n",
        "* RandomForestClassifier(max_depth=15,bootstrap=False,n_estimators=300)  \n",
        "* XGBClassifier(max_depth=8)  \n",
        "* SVC(C=13)  \n",
        "* VotingClassifier(estimators=[('svc', SVC(C=13)), ('rf', RandomForestClassifier(max_depth=17,n_estimators=300)), ('xgb', XGBClassifier(max_depth=6,n_estimators=300))], voting='hard')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e96Uk7FOVokd",
        "outputId": "4ae7a9f4-0718-4d40-daea-72eedbcaffcf"
      },
      "source": [
        "print(\"Random Forests\") #we were forced to manually do 3-fold cross validation due to resource constraints\n",
        "pr,rc,f1,model=cross_val_helper(RandomForestClassifier(max_depth=15,bootstrap=False,n_estimators=300),X,Y,train_kf[0],test_kf[0])\n",
        "Precision_RFC.append(pr) #storing the precision\n",
        "Recall_RFC.append(rc) #storing the recall\n",
        "F1_RFC.append(f1) #storing the F1-score\n",
        "models.append(model) #storing the model\n",
        "outfile=open(\"RFC TAV Dep.pkl\",\"wb\") #saving the scores as well as the models\n",
        "pickle.dump([Precision_RFC,Recall_RFC,F1_RFC,models],outfile)\n",
        "outfile.close()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Forests\n",
            "[69 28 44 89]\n",
            "Precision: 0.6898898973116342\n",
            "Recall: 0.6869565217391305\n",
            "F1: 0.6850484472049688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnXMngKNcbWI",
        "outputId": "fc819cbd-919d-4b67-e89a-bfa7309ae752"
      },
      "source": [
        "print(\"Random Forests\") #2nd fold\n",
        "pr,rc,f1,model=cross_val_helper(RandomForestClassifier(max_depth=15,bootstrap=False,n_estimators=300),X,Y,train_kf[1],test_kf[1])\n",
        "Precision_RFC.append(pr)\n",
        "Recall_RFC.append(rc)\n",
        "F1_RFC.append(f1)\n",
        "models.append(model)\n",
        "outfile=open(\"RFC TAV Dep.pkl\",\"wb\")\n",
        "pickle.dump([Precision_RFC,Recall_RFC,F1_RFC,models],outfile)\n",
        "outfile.close()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Forests\n",
            "[69 22 50 89]\n",
            "Precision: 0.7013161377252713\n",
            "Recall: 0.6869565217391305\n",
            "F1: 0.683617391304348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aocOa3gtccEC",
        "outputId": "5ed266c3-2c82-48ba-c088-44f1236982b5"
      },
      "source": [
        "print(\"Random Forests\") #3rd fold\n",
        "pr,rc,f1,model=cross_val_helper(RandomForestClassifier(max_depth=15,bootstrap=False,n_estimators=300),X,Y,train_kf[2],test_kf[2])\n",
        "Precision_RFC.append(pr)\n",
        "Recall_RFC.append(rc)\n",
        "F1_RFC.append(f1)\n",
        "models.append(model)\n",
        "outfile=open(\"RFC TAV Dep.pkl\",\"wb\")\n",
        "pickle.dump([Precision_RFC,Recall_RFC,F1_RFC,models],outfile)\n",
        "outfile.close()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Forests\n",
            "[71 29 42 88]\n",
            "Precision: 0.6931739130434783\n",
            "Recall: 0.691304347826087\n",
            "F1: 0.690007627765065\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRCe9EIQbv6o"
      },
      "source": [
        "weighted_pr=sum(Precision_RFC)/3 #calculating average weighted scores\n",
        "weighted_re=sum(Recall_RFC)/3\n",
        "weighted_f1=sum(F1_RFC)/3"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUs4-K8okJN0",
        "outputId": "206e77b3-2b82-406d-9058-fe77478e58c3"
      },
      "source": [
        "print(\"Random-Forest T+A+V Speaker Dependent\")\n",
        "print(\"Precision= \",weighted_pr)\n",
        "print(\"Recall= \",weighted_re)\n",
        "print(\"F1-score= \",weighted_f1)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random-Forest T+A+V Speaker Dependent\n",
            "Precision=  0.6947933160267946\n",
            "Recall=  0.6884057971014492\n",
            "F1-score=  0.6862244887581271\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}